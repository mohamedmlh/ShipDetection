{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import pandas as pd \n",
    "import torch \n",
    "import torchvision \n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000194a2d.jpg</td>\n",
       "      <td>360486 1 361252 4 362019 5 362785 8 363552 10 ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00021ddc3.jpg</td>\n",
       "      <td>108287 1 109054 3 109821 4 110588 5 111356 5 1...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002756f7.jpg</td>\n",
       "      <td>255784 2 256552 4 257319 7 258087 9 258854 12 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>000532683.jpg</td>\n",
       "      <td>458957 14 459725 14 460493 14 461261 14 462029...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0005d01c8.jpg</td>\n",
       "      <td>56010 1 56777 3 57544 6 58312 7 59079 9 59846 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>00113a75c.jpg</td>\n",
       "      <td>401790 1 402557 3 403325 5 404092 7 404859 9 4...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0017c19d6.jpg</td>\n",
       "      <td>329228 1 329995 3 330762 4 331529 6 332296 8 3...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>001aee007.jpg</td>\n",
       "      <td>496304 1 497071 3 497838 5 498605 7 499372 9 5...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>002943412.jpg</td>\n",
       "      <td>300511 1 301277 4 302043 6 302809 9 303576 11 ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>002c78530.jpg</td>\n",
       "      <td>258116 2 258884 4 259651 7 260419 6 261189 4 2...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>002fdcf51.jpg</td>\n",
       "      <td>373563 2 374331 4 375098 7 375866 9 376633 12 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>003520305.jpg</td>\n",
       "      <td>243266 4 244034 10 244802 14 245570 14 246337 ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>003b48a9e.jpg</td>\n",
       "      <td>291795 3 292559 7 293325 9 294093 10 294861 10...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0041d7084.jpg</td>\n",
       "      <td>89229 12 89997 12 90765 12 91533 12 92301 12 9...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>004331bef.jpg</td>\n",
       "      <td>243265 1 244031 4 244798 5 245565 7 246331 10 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ImageId                                      EncodedPixels  label\n",
       "3    000194a2d.jpg  360486 1 361252 4 362019 5 362785 8 363552 10 ...      5\n",
       "5    00021ddc3.jpg  108287 1 109054 3 109821 4 110588 5 111356 5 1...      9\n",
       "6    0002756f7.jpg  255784 2 256552 4 257319 7 258087 9 258854 12 ...      2\n",
       "11   000532683.jpg  458957 14 459725 14 460493 14 461261 14 462029...      2\n",
       "15   0005d01c8.jpg  56010 1 56777 3 57544 6 58312 7 59079 9 59846 ...      2\n",
       "45   00113a75c.jpg  401790 1 402557 3 403325 5 404092 7 404859 9 4...      7\n",
       "68   0017c19d6.jpg  329228 1 329995 3 330762 4 331529 6 332296 8 3...      2\n",
       "81   001aee007.jpg  496304 1 497071 3 497838 5 498605 7 499372 9 5...      4\n",
       "117  002943412.jpg  300511 1 301277 4 302043 6 302809 9 303576 11 ...      3\n",
       "133  002c78530.jpg  258116 2 258884 4 259651 7 260419 6 261189 4 2...      2\n",
       "143  002fdcf51.jpg  373563 2 374331 4 375098 7 375866 9 376633 12 ...      2\n",
       "151  003520305.jpg  243266 4 244034 10 244802 14 245570 14 246337 ...      7\n",
       "175  003b48a9e.jpg  291795 3 292559 7 293325 9 294093 10 294861 10...      2\n",
       "200  0041d7084.jpg  89229 12 89997 12 90765 12 91533 12 92301 12 9...      5\n",
       "204  004331bef.jpg  243265 1 244031 4 244798 5 245565 7 246331 10 ...      2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df=pd.read_csv(\"train_ship_segmentations_v2.csv\", index_col=0)\n",
    "df['label']=df[\"EncodedPixels\"].isna().apply(np.invert)*1\n",
    "aggregation={'EncodedPixels':lambda rle_codes: ' '.join(map(lambda x:str(x),rle_codes)),'label':sum}\n",
    "DF=df.groupby(\"ImageId\").agg(aggregation)\n",
    "DF=DF.reset_index()\n",
    "display(DF[DF['label']>1].head(15))\n",
    "\n",
    "\n",
    "trainfiles=os.listdir('train_v2')\n",
    "load_img = lambda filename: np.array(PIL.Image.open(f\"train_v2/{filename}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions pour le labelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from skimage.io import imread\n",
    "\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.measure import label, regionprops\n",
    "\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "def masks_as_image(in_mask_list, all_masks=None):\n",
    "    # Take the individual ship masks and create a single mask array for all ships\n",
    "    if all_masks is None:\n",
    "        all_masks = np.zeros((768, 768), dtype = np.int16)\n",
    "    #if isinstance(in_mask_list, list):\n",
    "    for mask in in_mask_list:\n",
    "        if isinstance(mask, str):\n",
    "            all_masks += rle_decode(mask)\n",
    "    return np.expand_dims(all_masks, -1)\n",
    "\n",
    "\n",
    "def bounding_box(rle_0):\n",
    "    #rle_0 =DF['EncodedPixels'].iloc[92]\n",
    "    mask_0 = rle_decode_bbox(rle_0)\n",
    "    lbl_0 = label(mask_0) \n",
    "    props = regionprops(lbl_0)\n",
    "    box=props[0].bbox\n",
    "    return box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU(bb1,bb2):    \n",
    "    assert bb1[1] < bb1[3]\n",
    "    assert bb1[0] < bb1[2]\n",
    "    assert bb2[1] < bb2[3]\n",
    "    assert bb2[0] < bb2[2]\n",
    "\n",
    "    # determine the coordinates of the intersection rectangle\n",
    "    x_left = max(bb1[1], bb2[1])\n",
    "    y_top = max(bb1[0], bb2[0])\n",
    "    x_right = min(bb1[3], bb2[3])\n",
    "    y_bottom = min(bb1[2], bb2[2])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    # The intersection of two axis-aligned bounding boxes is always an\n",
    "    # axis-aligned bounding box\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    # compute the area of both AABBs\n",
    "    bb1_area = (bb1[3] - bb1[1]) * (bb1[2] - bb1[0])\n",
    "    bb2_area = (bb2[3] - bb2[1]) * (bb2[2] - bb2[0])\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.0\n",
    "    return iou\n",
    "\n",
    "\n",
    "#chaine='003b48a9e.jpg'\n",
    "#bx=bounding_box(DF[DF[\"ImageId\"]==chaine]['EncodedPixels'].iloc[0])\n",
    "#bx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling-- Mettre les données en forme pour le forward du RPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Build anchors for all scales \n",
    "def labelling (ImageId,image,scale,rle):\n",
    "   scales={192:4,96:8,48:16,24:32}\n",
    "   fe_size=768//scales[scale]\n",
    "   index=0\n",
    "   ctr=np.zeros((scale*scale,2))\n",
    "   ctr_x = np.arange(scales[scale], (fe_size+1) * scales[scale], scales[scale])\n",
    "   ctr_y = np.arange(scales[scale], (fe_size+1) * scales[scale], scales[scale])\n",
    "   for x in range(len(ctr_x)):\n",
    "     for y in range(len(ctr_y)):\n",
    "       ctr[index,1]=ctr_x[x]-scales[scale]/2\n",
    "       ctr[index,0]=ctr_y[y]-scales[scale]/2\n",
    "       index+=1\n",
    "   anchors_boxes=np.zeros((scale*scale*3,4))\n",
    "   ratios=[0.5,1,2]\n",
    "   sub_sample=scales[scale]                     ######### Not sure \n",
    "   index=0\n",
    "   for c in ctr:\n",
    "     ctr_y,ctr_x=c\n",
    "     for i in range(len(ratios)):\n",
    "       h = sub_sample * np.sqrt(ratios[i])*scales[scale]\n",
    "       w = sub_sample * np.sqrt(1./ ratios[i])*scales[scale]\n",
    "       anchors_boxes[index, 0] = ctr_y - h / 2.\n",
    "       anchors_boxes[index, 1] = ctr_x - w / 2.\n",
    "       anchors_boxes[index, 2] = ctr_y + h / 2.\n",
    "       anchors_boxes[index, 3] = ctr_x + w / 2.\n",
    "       index += 1\n",
    "    \n",
    "   cls_labels=[]\n",
    "   for anchor in anchors_boxes:\n",
    "        #cls_labels=[Max_IOU_box(ImageId,anchor)[0] for anchor in anchors_boxes ]\n",
    "        temp=Max_IOU_box(ImageId,anchor)\n",
    "        \n",
    "        if temp[0] >=0.7:\n",
    "            anchor=temp[1]   ###  for the regressor, we replace the anchor box by the bounding_box of the object\n",
    "            cls_labels.append(1)\n",
    "        elif temp[0]<0.3:\n",
    "            cls_labels.append(-1)\n",
    "        else:\n",
    "            cls_labels.append(0)\n",
    "        #cls_labels=np.array(cls_labels)\n",
    "        #cls_labels[cls_labels>=0.7]=1\n",
    "        #cls_labels[cls_labels<0.3]=-1\n",
    "        #cls_labels[np.abs(cls_labels) !=1]=0\n",
    "   return (torch.Tensor(image),torch.Tensor(anchors_boxes),torch.Tensor(cls_labels))     ### We are not trying to predict exactly the bounding box but some approximation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from skimage.io import imread\n",
    "\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.measure import label, regionprops\n",
    "\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "def masks_as_image(in_mask_list, all_masks=None):\n",
    "    # Take the individual ship masks and create a single mask array for all ships\n",
    "    if all_masks is None:\n",
    "        all_masks = np.zeros((768, 768), dtype = np.int16)\n",
    "    #if isinstance(in_mask_list, list):\n",
    "    for mask in in_mask_list:\n",
    "        if isinstance(mask, str):\n",
    "            all_masks += rle_decode(mask)\n",
    "    return np.expand_dims(all_masks, -1)\n",
    "\n",
    "\n",
    "def bounding_box(ImageId):\n",
    "    rle_0 =DF[DF['ImageId']==ImageId]['EncodedPixels'].iloc[0]\n",
    "    mask_0 = rle_decode(rle_0)\n",
    "    lbl_0 = label(mask_0) \n",
    "    props = regionprops(lbl_0)\n",
    "    box=[]\n",
    "    for prop in props:\n",
    "        box.append(prop.bbox)\n",
    "    return box\n",
    "\n",
    "def Max_IOU_box(ImageId,anchor):\n",
    "    '''return the max of the IOU between the anchor and the box within the Image\n",
    "     and the index of this box'''\n",
    "    temp=bounding_box(ImageId)\n",
    "    temp2=[IOU(anchor,i) for i in temp]\n",
    "    temp2=np.array(temp2)\n",
    "    return (max(temp2),np.argmax(temp2))\n",
    "    \n",
    "#Max_IOU_box('003b48a9e.jpg',(716, 379, 726, 387))\n",
    "\n",
    "#df[df.index=='003b48a9e.jpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement des données avec labelling à chaque fois "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 100\n",
      "5 / 100\n",
      "10 / 100\n",
      "15 / 100\n",
      "20 / 100\n",
      "25 / 100\n",
      "30 / 100\n",
      "35 / 100\n",
      "40 / 100\n",
      "45 / 100\n"
     ]
    }
   ],
   "source": [
    "DF.index\n",
    "with_boat=list(DF[DF['label']!=0][\"ImageId\"].iloc[:])\n",
    "trdata = []\n",
    "Testdata = []\n",
    "og = 768\n",
    "s = 224\n",
    "counter1 = 0\n",
    "counter2=0\n",
    "nber_files=50\n",
    "\n",
    "  \n",
    "for filename in with_boat[:nber_files]:\n",
    "    im = load_img(filename)\n",
    "    \n",
    "    if df.query('ImageId==\"'+filename+'\"')['label'][0] >=1:\n",
    "        trdata.append(labelling(filename,torch.Tensor(im).permute(2,1,0),48,DF[DF['ImageId']==filename]['EncodedPixels'].iloc[0]))\n",
    "        \n",
    "    if (counter1%5==0):\n",
    "        filename2=with_boat[nber_files+counter1]\n",
    "        am=load_img(filename2)\n",
    "        Testdata.append(labelling(filename2,torch.Tensor(im).permute(2,1,0),48,DF[DF['ImageId']==with_boat[nber_files+counter1]]['EncodedPixels'].iloc[0]))\n",
    "        print(counter1, '/ 100')\n",
    "    counter1+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader=torch.utils.data.DataLoader(trdata,3)\n",
    "testloader=torch.utils.data.DataLoader(Testdata,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du backbone première partie -- Attention Mechanism and Spatial Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We are right after ResNet\n",
    "## we are impleting fpn with bottom-up augmented path\n",
    "## and CA or SA mechanism\n",
    "\n",
    "class CA_SA(torch.nn.Module):\n",
    "  def __init__(self,shape,batch=1,channel=256):\n",
    "    super(CA_SA,self).__init__()\n",
    "    self.channel=channel\n",
    "    self.batch=batch\n",
    "    self.shape=shape\n",
    "    self.residual=nn.Sequential(nn.Conv2d(self.channel,self.channel,kernel_size=3,padding=1,stride=1),nn.ReLU(),nn.Conv2d(self.channel,self.channel,kernel_size=3,padding=1,stride=1))\n",
    "    self.CA=nn.Sequential(nn.AvgPool2d(kernel_size=self.shape),\n",
    "                          nn.Conv2d(self.channel,self.channel,kernel_size=1),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(self.channel,self.channel,kernel_size=1),\n",
    "                          nn.Softmax(dim=1))\n",
    "    self.SA=nn.Sequential(nn.Conv2d(self.channel,1,kernel_size=3,stride=1,padding=1),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Conv2d(1,1,kernel_size=3,padding=1)\n",
    "                          )\n",
    "    ## We put the Softmax layer out :\n",
    "    self.actv=nn.Softmax(dim=2)\n",
    "    ## concatenate before this layer : torch.cat()\n",
    "    self.final_layer=nn.Conv2d(self.channel,self.channel,kernel_size=3,padding=1)\n",
    "    self.alpha=torch.ones((self.channel,1,1))\n",
    "    self.beta=torch.ones(self.shape)\n",
    "    self.conv=nn.Conv2d(2*self.channel,self.channel,kernel_size=3,padding=1)\n",
    "\n",
    "  def forward(self,inputs):\n",
    "    inputs=self.residual(inputs)\n",
    "    self.alpha=self.CA(inputs) #.view(self.batch,self.channel,1,1)   \n",
    "    ## multiplicate each channel by the corresponding component on alpha:\n",
    "    A=inputs*self.alpha\n",
    "    self.beta=self.SA(inputs)\n",
    "    self.beta=self.actv(self.beta)                ##### view_as à supp\n",
    "    \n",
    "    ## multiplicate each pixel by his corresponding coefficient in beta\n",
    "    B= inputs*self.beta\n",
    "   \n",
    "    ## concatenate \n",
    "    temp=torch.cat((A,B),dim=1)\n",
    "    #print(\"TEST\", temp.size())\n",
    "    temp=self.conv(temp)\n",
    "    return temp+inputs\n",
    "\n",
    "## Checking ok "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du backbone à proprement parler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "## building  of fpn structrure :\n",
    "class backbone(nn.Module):\n",
    "  def __init__(self,batch):\n",
    "    super().__init__()\n",
    "    ## We use resnet from mask_r_cnn pytorch's model\n",
    "    self.batch=batch\n",
    "    self.conv=model.backbone.body\n",
    "    self.first_conv=nn.Conv2d(2048,256,kernel_size=1)\n",
    "    self.up=nn.Upsample(scale_factor=2)\n",
    "    self.top_down=nn.ModuleList([nn.Conv2d(1024,256,kernel_size=1),\n",
    "                             nn.Conv2d(512,256,kernel_size=1),\n",
    "                             nn.Conv2d(256,256,kernel_size=1)])\n",
    "    self.AM1=nn.ModuleList([CA_SA(batch=self.batch,shape=(48,48)),\n",
    "                          CA_SA(batch=self.batch,shape=(96,96)),\n",
    "                          CA_SA(batch=self.batch,shape=(192,192))])\n",
    "    \n",
    "    self.down_top=nn.ModuleList([nn.Conv2d(256,256,kernel_size=3,padding=1,stride=2),\n",
    "                                nn.Conv2d(256,256,kernel_size=3,padding=1,stride=2),\n",
    "                                nn.Conv2d(256,256,kernel_size=3,padding=1,stride=2)])\n",
    "    \n",
    "    self.AM2=nn.ModuleList([CA_SA(batch=self.batch,shape=(96,96)),\n",
    "                           CA_SA(batch=self.batch,shape=(48,48)),\n",
    "                           CA_SA(batch=self.batch,shape=(24,24))])\n",
    "    \n",
    "  def forward(self,X):\n",
    "    resulta=[]\n",
    "    resultb=[]\n",
    "    TEMP=self.conv(X)\n",
    "    temp=TEMP['3']\n",
    "    temp=self.first_conv(temp)\n",
    "    resulta.append((\"P5\",temp))  ## Check if the value stay at it is or change after further modification\n",
    "    temp=self.up(temp)\n",
    "    index1=['2','1','0']\n",
    "    index2=['P4','P3','P2']\n",
    "    for i in range(0,len(self.top_down)):\n",
    "      \n",
    "      temp=temp+self.top_down[i](TEMP[index1[i]])\n",
    "      temp=self.AM1[i](temp)\n",
    "      \n",
    "      resulta.append((index2[i],temp))\n",
    "      temp=self.up(temp)\n",
    "      \n",
    "    resulta=OrderedDict(resulta)\n",
    "    ## augmented path:\n",
    "    index2=['P3','P4','P5']\n",
    "    index3= ['N3','N4','N5']\n",
    "    temp=resulta['P2']\n",
    "    resultb.append(('N2',temp))\n",
    "    for i in range(len(self.down_top)):\n",
    "      temp=self.down_top[i](temp)\n",
    "      #print(\"TEMP  \",temp.size())\n",
    "      temp=temp+resulta[index2[i]]\n",
    "      temp=self.AM2[i](temp)\n",
    "      resultb.append((index3[i],temp))\n",
    "    result=OrderedDict(resultb)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index=torch.topk(INPUT[2],1,dim=0)[1]\n",
    "#INPUT[1][index]\n",
    "torch.max(INPUT[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOC POUR LES LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import torch.nn.functional as F\n",
    "class SamePad2d(nn.Module):\n",
    "    \"\"\"Mimics tensorflow's 'SAME' padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(SamePad2d, self).__init__()\n",
    "        self.kernel_size = torch.nn.modules.utils._pair(kernel_size)\n",
    "        self.stride = torch.nn.modules.utils._pair(stride)\n",
    "\n",
    "    def forward(self, input):\n",
    "        in_width = input.size()[2]\n",
    "        in_height = input.size()[3]\n",
    "        out_width = math.ceil(float(in_width) / float(self.stride[0]))\n",
    "        out_height = math.ceil(float(in_height) / float(self.stride[1]))\n",
    "        pad_along_width = ((out_width - 1) * self.stride[0] +\n",
    "                           self.kernel_size[0] - in_width)\n",
    "        pad_along_height = ((out_height - 1) * self.stride[1] +\n",
    "                            self.kernel_size[1] - in_height)\n",
    "        pad_left = math.floor(pad_along_width / 2)\n",
    "        pad_top = math.floor(pad_along_height / 2)\n",
    "        pad_right = pad_along_width - pad_left\n",
    "        pad_bottom = pad_along_height - pad_top\n",
    "        return F.pad(input, (pad_left, pad_right, pad_top, pad_bottom), 'constant', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "    \"\"\"Builds the model of Region Proposal Network.\n",
    "    anchors_per_location: number of anchors per pixel in the feature map\n",
    "    anchor_stride: Controls the density of anchors. Typically 1 (anchors for\n",
    "                   every pixel in the feature map), or 2 (every other pixel).\n",
    "    Returns:\n",
    "        rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax)\n",
    "        rpn_probs: [batch, W, W, 2] Anchor classifier probabilities.\n",
    "        rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be\n",
    "                  applied to anchors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, anchors_per_location, anchor_stride, depth):\n",
    "        super(RPN, self).__init__()\n",
    "        self.anchors_per_location = anchors_per_location\n",
    "        self.anchor_stride = anchor_stride\n",
    "        self.depth = depth\n",
    "\n",
    "        self.padding = SamePad2d(kernel_size=3, stride=self.anchor_stride)\n",
    "        self.conv_shared = nn.Conv2d(self.depth, 256, kernel_size=3, stride=self.anchor_stride)   ### kernel_size  3---->16\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv_class = nn.Conv2d(256, 2 * anchors_per_location, kernel_size=1, stride=1)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.conv_bbox = nn.Conv2d(256, 4 * anchors_per_location, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shared convolutional base of the RPN\n",
    "        \n",
    "        x = self.relu(self.conv_shared(self.padding(x)))\n",
    "        \n",
    "\n",
    "        # Anchor Score. [batch, anchors per location * 2, height, width].\n",
    "        rpn_class_logits = self.conv_class(x)\n",
    "\n",
    "        # Reshape to [batch, anchors, 2]\n",
    "        rpn_class_logits = rpn_class_logits.permute(0,2,3,1)\n",
    "        rpn_class_logits = rpn_class_logits.contiguous()\n",
    "        rpn_class_logits = rpn_class_logits.view(x.size()[0], -1, 2)\n",
    "\n",
    "        # Softmax on last dimension of BG/FG.\n",
    "        rpn_probs = self.softmax(rpn_class_logits)\n",
    "\n",
    "        # Bounding box refinement. [batch, H, W, anchors per location, depth]\n",
    "        # where depth is [x, y, log(w), log(h)]\n",
    "        rpn_bbox = self.conv_bbox(x)\n",
    "\n",
    "        # Reshape to [batch, 4, anchors]\n",
    "        rpn_bbox = rpn_bbox.permute(0,2,3,1)\n",
    "        rpn_bbox = rpn_bbox.contiguous()\n",
    "        rpn_bbox = rpn_bbox.view(x.size()[0], -1, 4)\n",
    "\n",
    "        return [rpn_class_logits, rpn_probs, rpn_bbox]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rpn_class_loss(rpn_match, rpn_class_logits):\n",
    "    \"\"\"RPN anchor classifier loss.\n",
    "    rpn_match: [batch, anchors, 1]. Anchor match type. 1=positive,\n",
    "               -1=negative, 0=neutral anchor.\n",
    "    rpn_class_logits: [batch, anchors, 2]. RPN classifier logits for FG/BG.\n",
    "    \"\"\"\n",
    "\n",
    "    # Squeeze last dim to simplify\n",
    "    rpn_match = rpn_match.squeeze(0)    ### Modif \n",
    "    rpn_class_logits=rpn_class_logits.squeeze(0)  \n",
    "\n",
    "    # Get anchor classes. Convert the -1/+1 match to 0/1 values.\n",
    "    anchor_class = (rpn_match == 1).long()\n",
    "\n",
    "    # Positive and Negative anchors contribute to the loss,\n",
    "    # but neutral anchors (match value = 0) don't.\n",
    "    indices = torch.nonzero(rpn_match != 0)\n",
    "\n",
    "    # Pick rows that contribute to the loss and filter out the rest.\n",
    "    #rpn_class_logits = rpn_class_logits[indices.data[:,0],indices.data[:,1],:]\n",
    "    rpn_class_logits=rpn_class_logits[indices.data[:,0],indices.data[:,1]]\n",
    "    anchor_class = anchor_class[indices.data[:,0],indices.data[:,1]]\n",
    "\n",
    "    # Crossentropy loss\n",
    "    loss = F.cross_entropy(rpn_class_logits, anchor_class)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_rpn_bbox_loss(target_bbox, rpn_match, rpn_bbox):\n",
    "    \"\"\"Return the RPN bounding box loss graph.\n",
    "    target_bbox: [batch, max positive anchors, (dy, dx, log(dh), log(dw))].\n",
    "        Uses 0 padding to fill in unsed bbox deltas.\n",
    "    rpn_match: [batch, anchors, 1]. Anchor match type. 1=positive,\n",
    "               -1=negative, 0=neutral anchor.\n",
    "    rpn_bbox: [batch, anchors, (dy, dx, log(dh), log(dw))]\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Squeeze last dim to simplify\n",
    "    rpn_macth = rpn_match.squeeze(0)                   #### Modif \n",
    "    rpn_bbox=rpn_bbox.squeeze(0)\n",
    "\n",
    "    # Positive anchors contribute to the loss, but negative and\n",
    "    # neutral anchors (match value of 0 or -1) don't.\n",
    "    indices = torch.nonzero(rpn_match==1)\n",
    "    \n",
    "    # Pick bbox deltas that contribute to the loss\n",
    "    #rpn_bbox = rpn_bbox[indices.data[:,0],indices.data[:,1]]\n",
    "\n",
    "    #rpn_bbox.size())\n",
    "    rpn_bbox=rpn_bbox[indices.data[:,0],indices.data[:,1]]\n",
    "    \n",
    "\n",
    "    # Trim target bounding box deltas to the same length as rpn_bbox.\n",
    "    #target_bbox = target_bbox[0,:rpn_bbox.size()[0],:]\n",
    "    target_bbox=target_bbox[indices.data[:,0],indices.data[:,1]]\n",
    "\n",
    "    # Smooth L1 loss\n",
    "    loss = F.smooth_l1_loss(rpn_bbox, target_bbox)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, optimizer, train_loader, test_loader,scale,n_epoch = 2,\n",
    "          train_acc_period = 10,\n",
    "          test_acc_period = 5,\n",
    "          cuda=True, level=\"N4\"):\n",
    "  loss_train = []\n",
    "  loss_test = []\n",
    "  total = 0\n",
    "  for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
    "      torch.save(net,\"classifier_epoch\"+str(epoch)+\".pt\")\n",
    "      running_loss = 0.0\n",
    "      running_acc = 0.0\n",
    "      for i, data in enumerate(train_loader, 0):\n",
    "          # get the inputs\n",
    "          img, anchor_bbox,anchor_label= data\n",
    "          if cuda:\n",
    "            img = img.type(torch.cuda.FloatTensor)\n",
    "            #labels = labels.type(torch.cuda.LongTensor)\n",
    "          \n",
    "          # zero the parameter gradients\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          # forward + backward + optimize\n",
    "          img=step1(img)[level]\n",
    "          outputs = net(img)\n",
    "          logits,probs,pred_anchor_bbox=outputs\n",
    "          loss=compute_rpn_class_loss(anchor_label.type(torch.cuda.FloatTensor),logits)\n",
    "          loss1=loss+compute_rpn_bbox_loss(anchor_bbox.type(torch.cuda.FloatTensor),anchor_label.type(torch.cuda.FloatTensor),pred_anchor_bbox)\n",
    "          \n",
    "          \n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          #print(\"debut stack\")\n",
    "          Z2=probs.view(-1,2)\n",
    "          predicted=torch.Tensor([torch.argmax(i) for i in Z2])\n",
    "          #print(\" fin stack\")\n",
    "          labels = (anchor_label==1).long().view(-1)\n",
    "          total += labels.size(0)\n",
    "          # print statistics\n",
    "          running_loss = 0.33*loss.item()/3 + 0.66*running_loss\n",
    "          \n",
    "          correct = (predicted == labels).sum().item()/labels.size(0)\n",
    "          running_acc = 0.3*correct + 0.66*running_acc\n",
    "          if i % train_acc_period == train_acc_period-1:\n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss))\n",
    "            print('[%d, %5d] acc: %.3f' %(epoch + 1, i + 1, running_acc))\n",
    "            running_loss = 0.0\n",
    "            total = 0\n",
    "            # break\n",
    "      if epoch % test_acc_period == test_acc_period-1:\n",
    "          cur_acc= accuracy(net, test_loader, cuda=cuda)\n",
    "          #print('[%d] loss: %.3f' %(epoch + 1, cur_loss))\n",
    "          print('[%d] acc: %.3f' %(epoch + 1, cur_acc))\n",
    "      \n",
    "  print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.000\n",
      "[1,    10] acc: 0.868\n",
      "[2,    10] loss: 0.000\n",
      "[2,    10] acc: 0.868\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#step1=backbone(batch=3)\n",
    "#net=RPN(3,1,256)\n",
    "#net.cuda()\n",
    "#step1.cuda()\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate)\n",
    "train(net,optimizer,trainloader,testloader,scale=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy(net, testloader, cuda=True,level=\"N5\")\n",
    "torch.save(net,\"RPN_P5_first_step\"+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 3, 110592, 2), grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''indices=torch.nonzero(label.squeeze(0)==1)\n",
    "label=label.squeeze(0)\n",
    "boxes[indices.data[:,0],indices.data[:,1]]\n",
    "Z1[2].size()'''\n",
    "\n",
    "compute_rpn_class_loss(label,Z1[0])\n",
    "#torch.max(Z1[1],0)\n",
    "#label.view(-1,1)==\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(220155)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z2=Z1[1].view(3*110592,2)\n",
    "#a=torch.Tensor([torch.argmax(i) for i in Z1[1][0]])\n",
    "#b=torch.Tensor([torch.argmax(i) for i in Z1[1][1]])\n",
    "#c=torch.Tensor([torch.argmax(i) for i in Z1[1][2]])\n",
    "#fr=torch.stack([a,b,c],dim=0)\n",
    "#fr=torch.Tensor([torch.argmax(i) for i in Z2])\n",
    "#(fr==(label==1).long().view(3*110592)).sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rpn_class_loss(rpn_match, rpn_class_logits):\n",
    "    \"\"\"RPN anchor classifier loss.\n",
    "    rpn_match: [batch, anchors, 1]. Anchor match type. 1=positive,\n",
    "               -1=negative, 0=neutral anchor.\n",
    "    rpn_class_logits: [batch, anchors, 2]. RPN classifier logits for FG/BG.\n",
    "    \"\"\"\n",
    "\n",
    "    # Squeeze last dim to simplify\n",
    "    rpn_match = rpn_match.squeeze(0)    ### Modif \n",
    "    rpn_class_logits=rpn_class_logits.squeeze(0)  \n",
    "\n",
    "    # Get anchor classes. Convert the -1/+1 match to 0/1 values.\n",
    "    anchor_class = (rpn_match == 1).long()\n",
    "\n",
    "    # Positive and Negative anchors contribute to the loss,\n",
    "    # but neutral anchors (match value = 0) don't.\n",
    "    indices = torch.nonzero(rpn_match != 0)\n",
    "\n",
    "    # Pick rows that contribute to the loss and filter out the rest.\n",
    "    #rpn_class_logits = rpn_class_logits[indices.data[:,0],indices.data[:,1],:]\n",
    "    rpn_class_logits=rpn_class_logits[indices.data[:,0],indices.data[:,1]]\n",
    "    anchor_class = anchor_class[indices.data[:,0],indices.data[:,1]]\n",
    "\n",
    "    # Crossentropy loss\n",
    "    loss = F.cross_entropy(rpn_class_logits, anchor_class)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_rpn_bbox_loss(target_bbox, rpn_match, rpn_bbox):\n",
    "    \"\"\"Return the RPN bounding box loss graph.\n",
    "    target_bbox: [batch, max positive anchors, (dy, dx, log(dh), log(dw))].\n",
    "        Uses 0 padding to fill in unsed bbox deltas.\n",
    "    rpn_match: [batch, anchors, 1]. Anchor match type. 1=positive,\n",
    "               -1=negative, 0=neutral anchor.\n",
    "    rpn_bbox: [batch, anchors, (dy, dx, log(dh), log(dw))]\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Squeeze last dim to simplify\n",
    "    rpn_macth = rpn_match.squeeze(0)                   #### Modif \n",
    "    rpn_bbox=rpn_bbox.squeeze(0)\n",
    "\n",
    "    # Positive anchors contribute to the loss, but negative and\n",
    "    # neutral anchors (match value of 0 or -1) don't.\n",
    "    indices = torch.nonzero(rpn_match==1)\n",
    "    if len(indices)==0:\n",
    "        return torch.Tensor([0])\n",
    "    \n",
    "    # Pick bbox deltas that contribute to the loss\n",
    "    #rpn_bbox = rpn_bbox[indices.data[:,0],indices.data[:,1]]\n",
    "\n",
    "    #rpn_bbox.size())\n",
    "    rpn_bbox=rpn_bbox[indices.data[:,0],indices.data[:,1]]\n",
    "    \n",
    "\n",
    "    # Trim target bounding box deltas to the same length as rpn_bbox.\n",
    "    #target_bbox = target_bbox[0,:rpn_bbox.size()[0],:]\n",
    "    target_bbox=target_bbox[indices.data[:,0],indices.data[:,1]]\n",
    "\n",
    "    # Smooth L1 loss\n",
    "    loss = F.smooth_l1_loss(rpn_bbox, target_bbox)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(net, test_loader, cuda=True,level=\"N2\"):\n",
    "  net.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  loss = 0\n",
    "  with torch.no_grad():\n",
    "      for data in test_loader:\n",
    "          img, anchor_bbox,anchor_label= data\n",
    "          if cuda:\n",
    "            img = img.type(torch.cuda.FloatTensor)\n",
    "          img=step1(img)[level]\n",
    "          outputs = net(img)\n",
    "          logits,probs,pred_anchor_bbox=outputs\n",
    "          # loss+= criterion(outputs, labels).item()\n",
    "          Z2=probs.view(-1,2)\n",
    "          predicted=torch.Tensor([torch.argmax(i) for i in Z2])\n",
    "          labels = (anchor_label==1).long().view(-1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "          if total > 100:\n",
    "            break\n",
    "  net.train()\n",
    "  #print('Accuracy of the network on the test images: %d %%' % (\n",
    "     # 100 * correct / total))\n",
    "  # return (100.0 * correct / total, loss/total)\n",
    "  return 100.0 * correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BROUILLON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.index\n",
    "with_boat=list(DF[DF['label']!=0][\"ImageId\"].iloc[:])\n",
    "\n",
    "image_test=load_img('000532683.jpg')\n",
    "INPUT=labelling('000532683.jpg',image_test,48,DF['EncodedPixels'].iloc[11])\n",
    "#index=torch.where(INPUT[2]==1)\n",
    "#index[0].size()\n",
    "choisi=INPUT[1][index].squeeze(0)\n",
    "#choisi=bounding_box(DF['EncodedPixels'].iloc[175])\n",
    "import cv2\n",
    "img_clone=np.copy(image_test)\n",
    "plt.figure(figsize=(9,9))\n",
    "#for x in range(TEST):\n",
    "#for choisi in choisie:\n",
    "cv2.rectangle(img_clone,((int)(choisi[3]),(int)(choisi[2])),((int)(choisi[1]),(int)(choisi[0])),color=(255,0,0),thickness=2)\n",
    "plt.imshow(img_clone)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from skimage.io import imread\n",
    "\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.measure import label, regionprops\n",
    "\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "def masks_as_image(in_mask_list, all_masks=None):\n",
    "    # Take the individual ship masks and create a single mask array for all ships\n",
    "    if all_masks is None:\n",
    "        all_masks = np.zeros((768, 768), dtype = np.int16)\n",
    "    #if isinstance(in_mask_list, list):\n",
    "    for mask in in_mask_list:\n",
    "        if isinstance(mask, str):\n",
    "            all_masks += rle_decode(mask)\n",
    "    return np.expand_dims(all_masks, -1)\n",
    "\n",
    "\n",
    "def bounding_box(ImageId):\n",
    "    rle_0 =DF[DF['ImageId']==ImageId]['EncodedPixels'].iloc[0]\n",
    "    mask_0 = rle_decode(rle_0)\n",
    "    lbl_0 = label(mask_0) \n",
    "    props = regionprops(lbl_0)\n",
    "    box=[]\n",
    "    for prop in props:\n",
    "        box.append(prop.bbox)\n",
    "    return box\n",
    "\n",
    "def Max_IOU_box(ImageId,anchor):\n",
    "    '''return the max of the IOU between the anchor and the box within the Image\n",
    "     and the index of this box'''\n",
    "    temp=bounding_box(ImageId)\n",
    "    temp2=[IOU(anchor,i) for i in temp]\n",
    "    temp2=np.array(temp2)\n",
    "    return (max(temp2),np.argmax(temp2))\n",
    "    \n",
    "#Max_IOU_box('003b48a9e.jpg',(716, 379, 726, 387))\n",
    "\n",
    "#df[df.index=='003b48a9e.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(682, 44, 698, 68)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''re=[]\n",
    "for i in labelling(load_img(chaine),96,DF[DF[\"ImageId\"]==chaine]['EncodedPixels'].iloc[0])[1]:\n",
    "    re.append(IOU(bx,i))\n",
    "re=torch.Tensor(re)\n",
    "torch.max(re)'''\n",
    "a=labelling(chaine,load_img(chaine),24,DF[DF[\"ImageId\"]==chaine]['EncodedPixels'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step1=backbone(batch=3)\n",
    "net=RPN(3,1,256)\n",
    "#net.cuda()\n",
    "#step1.cuda()\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate)\n",
    "\n",
    "for j in trainloader:\n",
    "    i=next(iter(trainloader))\n",
    "    Z=step1(i[0])\n",
    "    Z1=net(Z['N5'])\n",
    "    #Z3=compute_rpn_class_loss(i[2],Z1[1])\n",
    "    Z4=compute_rpn_bbox_loss(i[1],i[2],Z1[2])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_default",
   "language": "python",
   "name": "conda-env-py37_default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
